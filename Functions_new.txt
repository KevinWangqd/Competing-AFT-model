

data_transform <- function(Y, delta, X, label) {
  # Check for NA values in inputs
  if (any(is.na(Y))) stop("Y contains NA values.")
  if (any(is.na(delta))) stop("delta contains NA values.")
  if (any(is.na(X))) stop("X contains NA values.")
  if (any(sapply(label, is.null))) stop("label contains NULL values.")
  
  # Check the same length
  if (length(Y) != length(delta)) {
    stop("Y and delta must have the same length.")
  }
  
  if (nrow(X) != length(Y)) {
    stop("The number of samples in X must match the length of Y.")
  }
  
  if (!is.list(label) || length(label) != ncol(X)) {
    stop("label must be a list of the same length as the number of columns in X.")
  }
  
  # Determine unique groups across all features
  groups <- unique(unlist(label))  # Extract unique group indices
  L <- length(groups)             # Number of competing factors
  
  # Initialize output lists
  X_list <- vector("list", L)    

  
  # Populate X_list for each group based on the label
  for (l in seq_along(groups)) {
    # Find features belonging to the current group
    group_features <- which(sapply(label, function(x) groups[l] %in% x))
    
    # Subset the corresponding features for the current group
    X_list[[l]] <- X[, group_features, drop = FALSE]
                    
  }
  
  # Return a list of transformed data and initialized parameters
  return(list(
    Y = Y,            
    delta = delta,    
    X = X_list 
  ))
}



param_init <- function(X, label, param = NULL) {
  groups <- unique(unlist(label))  # Extract unique group indices
  L <- length(groups)             # Number of competing factors
  
  # Initialize variables
  alpha <- vector("list", L)     
  beta <- vector("list", L)      
  sigma <- vector("list", L)     
  X_list <- vector("list", L) 
  
  # Check if param is provided
  if (is.null(param)) {
    # Populate X_list for each group based on the label
    for (l in seq_along(groups)) {
      group_features <- which(sapply(label, function(x) groups[l] %in% x))
      
      # Subset the corresponding features for the current group
      X_list[[l]] <- X[, group_features, drop = FALSE]
      
      # Initialize alpha, beta, and sigma for the current group
      ## The following is trivial. Consider better initialization if needed
      alpha[[l]] <- 1                              
      beta[[l]] <- rep(0, ncol(X_list[[l]]))        
      sigma[[l]] <- 1                             
    }
  } else {
    # Use provided param values
    alpha <- param$alpha
    beta <- param$beta
    sigma <- param$sigma
  }
  
  # Return a list of initialized parameters
  return(list(
    alpha = alpha,    
    beta = beta,      
    sigma = sigma
  ))
}




# The following is the main fit functions for EM algorithm
## Input two penalization lambda1 for alpha and lambda2 for beta

EM_fit <- function(transformed_data, init_param, lambda1 = 0.5, lambda2 = 0.2, maxit = 10^3, tolerance = 1e-6) {
  
  L = length(transformed_data$X)
  param_before <- vector("list", L) 
  
  # Initialize param_before with sigma, alpha, and beta lists
  for (l in 1:L) {
    param_before[[l]] = list(init_param$sigma[[l]], c(init_param$alpha[[l]], init_param$beta[[l]]))
  }
  
  param_new = param_before
  stop <- rep(FALSE, L)
  
  # Start the EM algorithm
  for (iter in 1:maxit) {
    
    # Compute the error matrix and eta_matrix (E-step)
    eta_matrix = eta_matrix_fun(Y = transformed_data$Y, delta = transformed_data$delta, 
                            X = transformed_data$X, 
                            alpha = lapply(param_new, function(x) x[[2]][1]), 
                            beta = lapply(param_new, function(x) x[[2]][-1]), 
                            sigma = lapply(param_new, function(x) x[[1]]))
    
    # Update param_new for each group (M-step)
    for (l in 1:L) {
      param_updater = update_l(Y = transformed_data$Y, delta = transformed_data$delta, 
                               eta_l = eta_matrix[, l], X_l = transformed_data$X[[l]], 
                               param_new[[l]], lambda1 = lambda1, lambda2 = lambda2)
      
      # Check convergence: if the difference in parameters is less than tolerance, stop the iteration for this group
      param_diff = sum((unlist(param_updater) - unlist(param_new[[l]]))^2)
      if (param_diff < tolerance) {
        stop[l] <- TRUE
      }
      
      param_new[[l]] = param_updater
    }
    
    # Check if all groups have converged
    if (all(stop)) {
      message(paste("Converged at iteration", iter))
      break
    }
  }
  
  # Return the final results
  res = list(param_new = param_new, eta_matrix = eta_matrix)
  return(res)
}

#result <- EM_fit(transformed_data,init_param, lambda1 = 1, lambda2 = 0.1, maxit = 10^3, tolerance = 1e-6)


#result$param_new




summary_EM_fit <- function(result, transformed_data) {
  # Extract parameters from param_new
  param <- result$param_new
  sigma_list <- lapply(param, function(x) x[[1]])
  alpha_list <- lapply(param, function(x) x[[2]][1]) 
  beta_list <- lapply(param, function(x) x[[2]][-1]) 
  
  # Get the number of groups
  L = length(transformed_data$X)
  
  # Create a list to store the standard errors for each group
  se_list <- list()
  se_sigma_list <- list()
  # Loop over each group to calculate information matrix and standard errors
  for (l in 1:L) {
    # Calculate the Fisher information matrix for group l
    information_matrix <- Inform_mat_l(Y = transformed_data$Y, 
                                       delta = transformed_data$delta, 
                                       X = transformed_data$X, 
                                       alpha = alpha_list, 
                                       beta = beta_list, 
                                       sigma = sigma_list, 
                                       l = l)
    
    # Calculate the standard errors
    se_list[[l]] <- se(information_matrix)
    
    se_sigma_list[[l]] <- se_sigma(Y = transformed_data$Y, 
                                    delta = transformed_data$delta, 
                                    X = transformed_data$X, 
                                    alpha = alpha_list, 
                                    beta = beta_list, 
                                    sigma = sigma_list, 
                                    l = l)
  }
  

  # Display results
  summary_output <- list()
  for (l in 1:L) {
    summary_output[[l]] <- list(
      "Group" = l,
      "Alpha" = alpha_list[[l]],
      "Beta" = beta_list[[l]],
      "Sigma" = sigma_list[[l]],
      "SE Alpha" = se_list[[l]][1],   # Standard error for alpha
      "SE Beta" = se_list[[l]][-1],    # Standard errors for beta
      "SE Sigma" = se_sigma_list[[l]][1]
    )
  }
  
  return(summary_output)
}


eta_matrix_fun <- function(Y, delta, X, alpha, beta, sigma){
  n <- length(Y)
  L = length(X)
  eta_matrix <- matrix(rep(NA,n*L), nrow=n)
  for (l in 1:L) {
    mu = alpha[[l]]+X[[l]]%*%beta[[l]]
    eta_matrix[,l] = exp((Y-mu)/sigma[[l]])*(1/sigma[[l]]) 
  } 
  
  sum_eta = rowSums(eta_matrix) ## calculate the denominator of eta_il
  eta_matrix = eta_matrix/sum_eta  
  return(eta_matrix)
}


transformed_survival_function <- function(y) {
  t <- exp(y)
  L <- length(X)
  sum <- 0
  for (l in 1:L) {
    mu_l <- alpha[[l]] + X[[l]] %*% beta[[l]]
    sum <- sum + (t / as.vector(exp(mu_l)))^(1 / sigma[[l]])
  }
  S <- exp(-sum) * t  # Include the Jacobian term (t = exp(y))
  return(S)
}

Ex_sur_time <- function(X, alpha, beta, sigma, method, M = 100, samples) {    ## M is the upper limit for definite integral
  # Nested survival function
  survival_function <- function(t) {
    L <- length(X)
    sum <- 0
    for (l in 1:L) {
      mu_l <- as.numeric(alpha[[l]] + X[[l]] %*% beta[[l]])
      sum <- sum + (t / exp(mu_l))^(1 / sigma[[l]])
    }
    S <- exp(-sum)  # Survival function
    return(S)
  }
  
  # Transformed survival function for importance sampling
  transformed_survival_function <- function(y) {
    t <- exp(y)  # Transform back to the original scale
    L <- length(X)
    sum <- 0
    
    for (l in 1:L) {
      mu_l <- alpha[[l]] + X[[l]] %*% beta[[l]]
      sum <- sum + (t / as.vector(exp(mu_l)))^(1 / sigma[[l]])
    }
    
    S <- exp(-sum)  # Survival function
    return(S * t)  # Include the Jacobian term (e^y)
  }
  
  # Nested Mill's Ratio residual function
  MR_res <- function(M) {
    L <- length(X)
    sum1 <- 0
    sum2 <- 0
    for (l in 1:L) {
      mu_l <- as.numeric(alpha[[l]] + X[[l]] %*% beta[[l]])
      sum1 <- sum1 + (M / exp(mu_l))^(1 / sigma[[l]])
      sum2 <- sum2 + M^((1 / sigma[[l]]) - 1) / (sigma[[l]] * exp(mu_l / sigma[[l]]))
    }
    res <- exp(-sum1) / sum2  # Mill's Ratio residual
    return(res)
  }
  
  # Importance Sampling function
  IS_exp <- function(samples) {
    weights <- survival_function(samples) / dexp(samples, rate = 1)  # Importance weights
    estimate <- mean(weights)  # Monte Carlo estimate of the integral
    return(estimate)
  }
  # Importance Sampling function
  IS_Weibull <- function(samples) {
    weights <- survival_function(samples) / dweibull(samples, shape = 2, scale = 2)  # Importance weights
    estimate <- mean(weights)  # Monte Carlo estimate of the integral
    return(estimate)
  }
  
  
  IS_gumbel <- function(y_samples) {
    # Calculate weights
    weights <- transformed_survival_function(y_samples)/dgumbel(y_samples, loc = 0, scale = 1)
    estimate <- mean(weights)  # Monte Carlo estimate of the integral
    return(estimate)
  }
  
  if (method == "MR") {
    # For Mill's ratio and numerical approximation
    # Integrate the survival function up to M
    definite <- integrate(survival_function, lower = 0, upper = M, subdivisions = 10^8)$value
    
    # Calculate the Mill's ratio residual
    residual <- MR_res(M)
    
    # Expected survival time
    Ex_time <- definite + residual
  } 
  else if (method == "IS_exp") {
    # Use importance sampling for the integral
    Ex_time <- IS_exp(samples)
  } 
  else if (method == "IS_Weibull") {
    # Use importance sampling for the integral
    Ex_time <- IS_Weibull(samples)
  } 
  else if (method == "IS_gumbel") {  # Importance Sampling with Gumbel distribution
    Ex_time <- IS_gumbel(samples)
  } 
  else {
    stop("Invalid method. Use 'MR' for Mills Ratio or 'IS' for Importance Sampling.")
  }
  
  
  
  return(Ex_time)
}




sur_time_all <- function(result, transformed_data, method = "IS_exp", M = 100, n_samples = 10^6) {
  sigma_list <- lapply(result$param_new, function(x) x[[1]])
  alpha_list <- lapply(result$param_new, function(x) x[[2]][1])
  beta_list <- lapply(result$param_new, function(x) x[[2]][-1])
  
  n <- length(transformed_data$X[[1]][,1])
  
  Ex_time <- numeric(n)
  
  
  if (method == "MR") {
    for (i in seq_len(n)) {
      X <- lapply(transformed_data$X, function(x) x[i,])    # Efficient extraction
      Ex_time[i] <- Ex_sur_time(X = X, alpha = alpha_list, beta = beta_list, 
                                sigma = sigma_list, method = method, M = M, samples = 1)
    }
  } 
  else if (method == "IS_exp") {
  rate <- 1
  samples <- rexp(n_samples, rate = rate) 
  for (i in seq_len(n)) {
    X <- lapply(transformed_data$X, function(x) x[i,])    # Efficient extraction
    Ex_time[i] <- Ex_sur_time(X = X, alpha = alpha_list, beta = beta_list, 
                              sigma = sigma_list, method = method, M = M, samples = samples)
  }
  
  } 
  else if (method == "IS_Weibull") {
  samples <- rweibull(n_samples, shape = 2, scale = 2)  
  for (i in seq_len(n)) {
    X <- lapply(transformed_data$X, function(x) x[i,])    # Efficient extraction
    Ex_time[i] <- Ex_sur_time(X = X, alpha = alpha_list, beta = beta_list, 
                              sigma = sigma_list, method = method, M = M, samples = samples)
  }
  } 
  else if (method == "IS_gumbel") { 
  y_samples <- rgumbel(n_samples, loc = 0, scale = 1)
  for (i in seq_len(n)) {
    X <- lapply(transformed_data$X, function(x) x[i,])    # Efficient extraction
    Ex_time[i] <- Ex_sur_time(X = X, alpha = alpha_list, beta = beta_list, 
                              sigma = sigma_list, method = method, M = M, samples = samples)
  }
  } 
  else {
    stop("Invalid method. Use 'MR' for Mills Ratio or 'IS' for Importance Sampling.")
  }
  

  
  return(Ex_time)
}


Inform_mat_l <- function(Y, delta, X, alpha, beta, sigma, l){
  n <- length(Y)                        ## num of samples
  L = length(X)
  
  ## num of competing groups
  mu = alpha[[l]]+X[[l]]%*%beta[[l]]
  temp = exp((Y-mu)/sigma[[l]])                       ## S_il in Appendix. vector of n*1

  eta_matrix <- matrix(rep(NA,n*L), nrow=n)
  for (m in 1:L) {
    mu = alpha[[m]]+X[[m]]%*%beta[[m]]
    eta_matrix[,m] = exp((Y-mu)/sigma[[m]])*(1/sigma[[m]]) 
  } 
  sum_eta = rowSums(eta_matrix) 
  
  ## Now compute the three terms in 2-derivative (for alpha_l)
  ## every term is a n*1 vector
  term1 = eta_matrix[,l]/sum_eta/sigma[[l]]/sigma[[l]]
  term2 = -(eta_matrix[,l]/sum_eta/sigma[[l]])^2
  term3 = -temp/sigma[[l]]/sigma[[l]]
  
  d2_log_lik = (term1+term2)*delta + term3  ## 2-derivative for each sample, vector of n*1
  
  # Calculate the sign and magnitude of d2_log_lik
  sign_vec = sign(d2_log_lik)  # Extract the sign (-1, 0, 1 for each element)
  magnitude_vec = abs(d2_log_lik)  # Take the absolute value of d2_log_lik
  
  # Reconstruct the diagonal matrix using the original signs
  diag_mat_1 = diag(as.numeric(sign_vec * sqrt(magnitude_vec)), nrow = n)
  diag_mat_2 = diag(as.numeric(sqrt(magnitude_vec)), nrow = n)
  # Combine the intercept column with the feature matrix
  X_l = cbind(rep(1, n), X[[l]])
  
  # Compute the weighted feature matrix
  mat_X_l_1 = diag_mat_1 %*% X_l
  mat_X_l_2 = diag_mat_2 %*% X_l
  # Compute the information matrix
  information_matrix = t(mat_X_l_1) %*% mat_X_l_2
  information_matrix = -information_matrix
  return(information_matrix)
}





se <- function(matrix){
  inv_mat = solve(matrix, tol=1e-5)  ## calculate 
  se = sqrt(abs(diag(inv_mat)))
  return(se)
}



se_sigma <- function(Y, delta, X, alpha, beta, sigma, l){
  n <- length(Y)                        ## num of samples
  L = length(X)
  
  ## num of competing groups
  mu = alpha[[l]]+X[[l]]%*%beta[[l]]
  temp1 = (Y-mu)/sigma[[l]]
  
  eta_matrix <- matrix(rep(NA,n*L), nrow=n)
  for (m in 1:L) {
    mu = alpha[[m]]+X[[m]]%*%beta[[m]]
    eta_matrix[,m] = exp((Y-mu)/sigma[[m]])*(1/sigma[[m]]) 
  } 
  sum_eta = rowSums(eta_matrix) 
  
  term1 = (eta_matrix[,l]/sum_eta/sigma[[l]]/sigma[[l]])*(temp1^2+4*temp1+2)
  term2 = -((eta_matrix[,l]/sum_eta/sigma[[l]])*(1+temp1))^2
  term3 = -2*temp1*exp(temp1)/sigma[[l]]/sigma[[l]] - temp1^2*exp(temp1)/sigma[[l]]/sigma[[l]]
  
  d2_log_lik = (term1+term2)*delta + term3
  sum = sum(d2_log_lik)
  se = sqrt(abs(1/sum))
  return(se)
}



update_l <- function(Y, delta, eta_l, X_l, param_l_old, lambda1, lambda2){
  len_l = length(param_l_old[[2]])   ## length of alpha_l+ beta_l
  param_l_new = param_l_old
  
  res = coordinate_update(j = 0, Y, delta, eta_l, X_l, alpha_l= param_l_old[[2]][1], beta_l = param_l_old[[2]][2:len_l], 
                            sigma_l = param_l_old[[1]], lambda1, lambda2)
  param_l_new[[2]][1] = res    ## the list start from 1 instead of 0
  
  
  res = coordinate_update(j = 1, Y, delta, eta_l, X_l, alpha_l = param_l_old[[2]][1], beta_l = param_l_old[[2]][2:len_l], 
                            sigma_l = param_l_old[[1]], lambda1, lambda2)
  param_l_new[[2]][2:len_l] = res    


  
  res_sigma = coordinate_update(j = -1, Y, delta, eta_l, X_l, alpha_l= param_l_new[[2]][1], beta_l = param_l_new[[2]][2:len_l], 
                                sigma_l = param_l_new[[1]], lambda1, lambda2)
  
  param_l_new[[1]] = res_sigma  ## update sigma
  
  return(param_l_new)
}



coordinate_update <- function(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2) {
  
  # Update sigma_l
  if(j==-1){
    
    initial_sigma <- sigma_l    
    lb <- max(0.001, sigma_l - 0.01)  
    ub <- sigma_l + 0.01             
    
    # Bundle additional parameters into a list
    args_sigma <- list(j = -1, Y = Y, delta = delta, eta_l = eta_l, X_l = X_l,
                       alpha_l = alpha_l, beta_l = beta_l, lambda1 = lambda1, lambda2 = lambda2)
    
    # Use optim with L-BFGS-B method to respect bounds
    result <- optim(
      par = initial_sigma,
      fn = function(sigma_l) {
        with(args_sigma, Q_l(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2))
      },
      gr = function(sigma_l) {
        with(args_sigma, dQ_l_sigma(Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l))
      },
      method = "L-BFGS-B",
      lower = lb,
      upper = ub,
      control = list(pgtol = 1e-6)
    )
    
    return(result$par)
    
  }
  
  # Update alpha
  else if(j==0){
    initial_alpha <- alpha_l     # Starting value for alpha_l
    
    args_alpha <- list(j = 0, Y = Y, delta = delta, eta_l = eta_l,  X_l = X_l,
                       beta_l = beta_l, sigma_l = sigma_l, lambda1 = lambda1, lambda2 = lambda2)
    
    result_alpha <- optim(
      par = initial_alpha,
      fn = function(alpha_l) {
        with(args_alpha, Q_l(j = 0, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2))
      },
      gr = function(alpha_l) {
        with(args_alpha, dQ_l(j = 0, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2))
      },
      method = "BFGS",
      control = list(reltol = 1e-3)
    )
    return(result_alpha$par)
    
  }
  
  else{
    
    initial_beta <- beta_l
    
    #dQ_l_0 <- dQ_l(j, Y, delta, eta_l, X_l, alpha_l, construct_beta(0), sigma_l, lambda1 = 0, lambda2 = 0)
    
    # Set beta_lj to 0 if the derivative condition is met
    #if (abs(dQ_l_0) < lambda2) {
    #  return(0)   ## set beta_lj = 0
    #}
    
    # Arguments for optimization
    args_beta <- list(j = 1, Y = Y, delta = delta, eta_l = eta_l, X_l = X_l,
                      alpha_l = alpha_l, sigma_l = sigma_l,
                      lambda1 = lambda1, lambda2 = lambda2)
    
    # Perform optimization using `optim`
    result_beta <- optim(
      par = beta_l,
      fn = function(beta_l) {
        with(args_beta, Q_l(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2))
      },
      gr = function(beta_l) {
        with(args_beta, dQ_l(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2))
      },
      method = "BFGS",
      control = list(reltol = 1e-3)
    )
    
    beta_l_opt = result_beta$par
    
    # Step 2: Screening to enforce sparsity (for each beta_l[j])
    for (k in 1:length(beta_l_opt)) {
    
    
     if (length(beta_l_opt) == 1) {
       beta_temp = 0
     } else if (k == 1) {
    # Update the first element of beta_l
        beta_temp = c(0, beta_l_opt[(k + 1):length(beta_l_opt)])
      } else if (k == length(beta_l_opt)) {
    # Update the last element of beta_l
       beta_temp = c(beta_l_opt[1:(k - 1)], 0)
      } else {
    # Update an intermediate element of beta_l
       beta_temp = c(beta_l_opt[1:(k - 1)], 0, beta_l_opt[(k + 1):length(beta_l_opt)])
     }
    
     dQ_at_0 <- dQ_l(1, Y, delta, eta_l, X_l, alpha_l, beta_temp, sigma_l, lambda1 = 0, lambda2 = 0)
    
     if (abs(dQ_at_0[k]) < lambda2) {
       beta_l_opt[k] <- 0
     }

    }
    
    # Return the optimized beta_lj
    return(beta_l_opt)
    
    
  }
    
    

  
  
}



# Score functions for coordinate_update

## Sec 3: Penalization
# Q function 
#### This function can be simplified in practice (for speed) for est in alpha, beta, sigma. we can only keep the related terms in gradient descending.#####


Q_l <- function(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2){
  mu_l = alpha_l+X_l%*%beta_l      
  temp = (Y-mu_l)/sigma_l
  s = -sum( delta*eta_l*(temp-log(sigma_l))-exp(temp))
  if(j==-1){
    s = s
  }
  else if(j==0){
    s = s + lambda1*exp(-alpha_l)
  }
  else{
    s = s + lambda2*sum(abs(beta_l))
  }
  
  return(s)
}


## Derivatives for alpha_l and beta_lj
dQ_l <- function(j, Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l, lambda1, lambda2){
  mu_l = alpha_l+X_l%*%beta_l
  temp = exp((Y-mu_l)/sigma_l)
  
  s = 0
  
  if(j==0){
    s = (1/sigma_l)*sum(delta*eta_l-temp)-lambda1*exp(-alpha_l)
  }
  else{
    s= (1/sigma_l)*colSums(X_l*as.vector(delta*eta_l-temp) )+lambda2*sign(beta_l)
  }
  return(s)
}






## Derivatives for sigma_l 
dQ_l_sigma <- function(Y, delta, eta_l, X_l, alpha_l, beta_l, sigma_l){
  mu_l = alpha_l+X_l%*%beta_l  
  
  temp = exp((Y-mu_l)/sigma_l)
  
  s = (-1/sigma_l^2)*sum(delta*eta_l*(mu_l-Y-sigma_l) + (Y-mu_l)*temp)
  
  return(s)
}
